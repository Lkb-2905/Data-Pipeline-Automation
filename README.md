# ğŸš‚ Freight Data Pipeline Automation V1.0
ğŸŒ Camrail / BollorÃ© Logistics Data Engineering Project
![Python](https://img.shields.io/badge/Python-3.12-blue) ![SQLite](https://img.shields.io/badge/SQLite-Data_Warehouse-lightgrey) ![Pandas](https://img.shields.io/badge/Pandas-Transformation-green)

**Version:** 1.0.0 Stable | **Date:** FÃ©vrier 2026  
**Auteur:** KAMENI TCHOUATCHEU GAETAN BRUNEL  

---

## ğŸ¯ VUE D'ENSEMBLE DU PROJET

Ce projet illustre de solides compÃ©tences en **IngÃ©nierie des DonnÃ©es (Data Engineering)** en automatisant le traitement quotidien des flux logistiques de fret (locomotives, marchandises, incidents en gare).

âœ… **ETL Architecture :** ModÃ¨le complet (Extraction, Transformation, Chargement).
âœ… **Data Warehouse :** Alimentation transactionnelle d'une base de donnÃ©es SQLite3.
âœ… **Automatisation O.S :** PrÃ©paration pour la mise en tÃ¢che planifiÃ©e Windows (Task Scheduler).

| Aspect | DÃ©monstration |
| :--- | :--- |
| **Gouvernance IT** | Consolidation des donnÃ©es dispersÃ©es vers une base relationnelle. |
| **ScalabilitÃ©** | Scripts sÃ©parÃ©s (`extract.py`, `transform.py`, `load.py`). |
| **FiabilitÃ©** | ExÃ©cution orchestrÃ©e supportant la gestion d'exceptions globale. |
| **Business Value** | KPI calculÃ©s (Alertes logistiques, Volume par gare). |

---

## ğŸ—ï¸ ARCHITECTURE DU PIPELINE

1. **Layer: Extraction** (`extract.py`)
   * Connexion simulÃ©e au RÃ©fÃ©rentiel Machines (API/JSON).
   * RÃ©cupÃ©ration des transactions de fret journalier.
2. **Layer: Transformation** (`transform.py`)
   * Normalisation des dates et jointure (Merge) des tables de faits/dimensions.
   * AgrÃ©gation analytique par gare de triage.
3. **Layer: Loading** (`load.py` & `main_pipeline.py`)
   * Chargement par lot via SQLAlchemy dans `supply_chain_dwh.sqlite`.

---

## ğŸš€ DÃ‰MARRAGE RAPIDE

```bash
# 1. Naviguer dans le dossier du projet
cd Data-Pipeline-Automation

# 2. CrÃ©er l'environnement
python -m venv env
.\env\Scripts\activate

# 3. Installer les dÃ©pendances
pip install -r requirements.txt

# 4. Lancer le pipeline complet
python src/main_pipeline.py
```

Le fichier `database/supply_chain_dwh.sqlite` sera gÃ©nÃ©rÃ©, regorgeant des tables de faits propres.

---

## ğŸ“– REQUÃŠTAGE ET AUTOMATISATION

Le fichier `sql/advanced_queries.sql` met en valeur des requÃªtes SQL avancÃ©es avec les fenÃªtres temporelles analytiques (Window Functions `LAG()`, `OVER()`) pour classer la fiabilitÃ© des Hubs.
Le script `automate_etl.ps1` sert au dÃ©ploiement de routine nocturne sur serveur de production.

Â© 2026 Kameni Tchouatcheu Gaetan Brunel - Tous droits rÃ©servÃ©s
